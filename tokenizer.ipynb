{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5Ydu+f3yyW9SgcZB/9+ot",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stkao05/bpe/blob/main/tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stat(tokens, stats=None):\n",
        "  stats = {} if stats is None else stats\n",
        "  for pair in zip(tokens, tokens[1:]):\n",
        "    stats[pair] = stats.get(pair, 0) + 1\n",
        "  return stats\n",
        "\n",
        "\n",
        "def merge(tokens, pair, idx):\n",
        "  tkn = []\n",
        "  i = 0\n",
        "  while i < len(tokens):\n",
        "    if i + 1 < len(tokens) and (tokens[i], tokens[i + 1]) == pair:\n",
        "      tkn.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      tkn.append(tokens[i])\n",
        "      i += 1\n",
        "\n",
        "  return tkn\n",
        "\n",
        "text = \"hello world hello\"\n",
        "tokens = list(text.encode(\"utf-8\"))\n",
        "\n",
        "merges = {}\n",
        "\n",
        "src_vocab_size = 256\n",
        "vocab_size = 259\n",
        "\n",
        "n = vocab_size - src_vocab_size\n",
        "idx = src_vocab_size\n",
        "\n",
        "for _ in range(n):\n",
        "  stat = get_stat(tokens)\n",
        "  top_pair = sorted(stat, key=stat.get, reverse=True)[0]\n",
        "  tokens = merge(tokens, top_pair, idx)\n",
        "  merges[top_pair] = idx\n",
        "  idx += 1\n",
        "\n",
        "tokens, merges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csLV01aKXSUZ",
        "outputId": "83ff8f8e-ae33-44d5-c75e-67b7a950735d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([258, 111, 32, 119, 111, 114, 108, 100, 32, 258, 111],\n",
              " {(104, 101): 256, (256, 108): 257, (257, 108): 258})"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicTokenizer:\n",
        "\n",
        "  def train(self, text, vocab_size, verbose=False):\n",
        "    assert vocab_size > 256\n",
        "\n",
        "    self.merges = {}\n",
        "    self.vocab = {i:bytes([i]) for i in range(256)}\n",
        "    tokens = list(text.encode(\"utf-8\"))\n",
        "    num_merge = vocab_size - 256\n",
        "\n",
        "    for i in range(num_merge):\n",
        "      stat = get_stat(tokens)\n",
        "      if len(stat) == 0: # single token\n",
        "        break\n",
        "\n",
        "      idx = 256 + i\n",
        "      top_pair = max(stat, key=stat.get)\n",
        "      tokens = merge(tokens, top_pair, idx)\n",
        "\n",
        "      p0, p1 = top_pair\n",
        "      self.merges[top_pair] = idx\n",
        "      self.vocab[idx] = self.vocab[p0] + self.vocab[p1]\n",
        "\n",
        "      if verbose:\n",
        "        print(f\"merge {top_pair} -> {idx}\")\n",
        "\n",
        "\n",
        "  def encode(self, text):\n",
        "    tokens = list(text.encode(\"utf-8\"))\n",
        "    for pair, idx in self.merges.items():\n",
        "      tokens = merge(tokens, pair, idx)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "  def decode(self, ids):\n",
        "    bs = b''.join([self.vocab[id] for id in ids])\n",
        "    txt = bs.decode('utf-8', errors=\"replace\")\n",
        "    return txt\n",
        "\n",
        "\n",
        "basic_tkr = BasicTokenizer()\n",
        "basic_tkr.train(\"abc! abc! abc! hello hello world steven\", 300, verbose=False)\n",
        "basic_tkr.decode(basic_tkr.encode(\"abc!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "swQE003NcHmy",
        "outputId": "86390b8c-e068-493c-b3b8-891dfaa51029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abc!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: download text from https://raw.githubusercontent.com/karpathy/minbpe/master/tests/taylorswift.txt into a string var\n",
        "\n",
        "!curl -s https://raw.githubusercontent.com/karpathy/minbpe/master/tests/taylorswift.txt > taylor_swift.txt\n",
        "with open(\"taylor_swift.txt\") as f:\n",
        "  txt = f.read()\n"
      ],
      "metadata": {
        "id": "8mIDp5fUNnUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tkr = BasicTokenizer()\n",
        "tkr.train(txt, 300, verbose=False)"
      ],
      "metadata": {
        "id": "qewu9BkhhpCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utf_tokens = list(txt.encode('utf-8'))\n",
        "bpe_tokens = tkr.encode(txt)\n",
        "\n",
        "print(\"before:\", len(utf_tokens))\n",
        "print(\"after:\", len(bpe_tokens))\n",
        "print(\"comprssion\", len(utf_tokens) / len(bpe_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLEEramKld1n",
        "outputId": "2011c6c0-a411-4481-ea55-ba8d72b9ecbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before: 185768\n",
            "after: 128451\n",
            "comprssion 1.4462168453340185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RegexTokenizer"
      ],
      "metadata": {
        "id": "D8uQCZD0LoKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "\n",
        "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "\n",
        "re.findall(GPT4_SPLIT_PATTERN, \" hello world!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D044cnB2Tb1",
        "outputId": "1734c9d6-f622-446f-e4c2-99c2a5986031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' hello', ' world', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_between(lst, element):\n",
        "  \"\"\"\n",
        "  insert an element in between the list\n",
        "  ex:\n",
        "  insert_between([1,2,3], \",\") -> [1, \",\", 2, \",\", 3]\n",
        "  \"\"\"\n",
        "  return [e for i in lst for e in (i, element)][:-1]\n",
        "\n",
        "\n",
        "def segment(s, spliter):\n",
        "  \"\"\"\n",
        "  split a string into list of string\n",
        "  partition(\"a,cd\", \",\") -> [\"a\", \",\", \"cd\"]\n",
        "  \"\"\"\n",
        "  ls = s.rsplit(spliter)\n",
        "  ls = insert_between(ls, spliter)\n",
        "  return ls\n",
        "\n",
        "\n",
        "def enc(chunk, special_tokens):\n",
        "  \"\"\"\n",
        "  given a chunk string, return a list of id\n",
        "\n",
        "  chunk is a string that is either a speical token\n",
        "  string or a string not does not contain any speical token\n",
        "  \"\"\"\n",
        "  if chunk in special_tokens:\n",
        "    return [special_tokens[chunk]]\n",
        "  else:\n",
        "    return list(chunk.encode(\"utf-8\"))\n",
        "\n",
        "\n",
        "special_tokens = {\n",
        "  \"<|endoftext|>\": 55555,\n",
        "  \"<|startoftext|>\": 66666,\n",
        "}\n",
        "\n",
        "text = \"<|startoftext|>abc<|endoftext|>\"\n",
        "chunks = [text]\n",
        "\n",
        "for token in special_tokens:\n",
        "  chunks = [si for s in chunks for si in segment(s, token)]\n",
        "\n",
        "ids = [id for c in chunks for id in enc(c, special_tokens)]\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90Fiek6J8e78",
        "outputId": "33feebbd-a90e-4dde-ffb6-d439152e471b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[66666, 97, 98, 99, 55555]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RegexTokenizer(BasicTokenizer):\n",
        "\n",
        "  def __init__(self,\n",
        "               split_pattern=None,\n",
        "               special_tokens=None):\n",
        "    \"\"\"\n",
        "    special_tokens: a dict that maps special token string to a id\n",
        "    ex:\n",
        "    { \"<|endoftext|>\": 55555 }\n",
        "    \"\"\"\n",
        "    self.split_pattern = split_pattern\n",
        "    self.special_tokens = {} if special_tokens == None else special_tokens\n",
        "\n",
        "\n",
        "  def train(self, text, vocab_size, verbose=False):\n",
        "    assert vocab_size > 256\n",
        "\n",
        "    self.merges = {}\n",
        "    self.vocab = {i:bytes([i]) for i in range(256)}\n",
        "\n",
        "    text_list = re.findall(GPT4_SPLIT_PATTERN, text)\n",
        "    ids_list = [list(t.encode(\"utf-8\")) for t in text_list]\n",
        "    num_merge = vocab_size - 256\n",
        "\n",
        "    for i in range(num_merge):\n",
        "      stat = {}\n",
        "      for ids in ids_list:\n",
        "        stat = get_stat(ids, stat)\n",
        "\n",
        "      if len(stat) == 0: break\n",
        "      idx = 256 + i\n",
        "      top_pair = max(stat, key=stat.get)\n",
        "      ids_list = [merge(ids, top_pair, idx) for ids in ids_list]\n",
        "\n",
        "      p0, p1 = top_pair\n",
        "      self.merges[top_pair] = idx\n",
        "      self.vocab[idx] = self.vocab[p0] + self.vocab[p1]\n",
        "\n",
        "      if verbose:\n",
        "        print(f\"merge ({self.vocab[p0]}, {self.vocab[p1]}) -> {self.vocab[idx]} | {top_pair} -> {idx}\")\n",
        "\n",
        "\n",
        "  def encode(self, text):\n",
        "    \"\"\"\n",
        "    text -> list of int\n",
        "    \"\"\"\n",
        "    # first split text into token or non-token chunk i.e. [\"a\", \"<start>\", \"b\"]\n",
        "    chunks = [text]\n",
        "    for token in special_tokens:\n",
        "      chunks = [si for s in chunks for si in segment(s, token)]\n",
        "\n",
        "    # encode each chunk. make sure special tokens are encoded [97, 55555, 98]\n",
        "    ids = [id for c in chunks for id in enc(c, special_tokens)]\n",
        "\n",
        "    # BPE\n",
        "    for pair, idx in self.merges.items():\n",
        "      ids = merge(ids, pair, idx)\n",
        "\n",
        "    return ids\n",
        "\n",
        "\n",
        "  def decode(self, ids):\n",
        "    bs = b''.join([self.vocab[id] for id in ids])\n",
        "    txt = bs.decode('utf-8', errors=\"replace\")\n",
        "    return txt\n",
        "\n",
        "\n",
        "special_tokens = {\n",
        "  \"<|endoftext|>\": 55555,\n",
        "  \"<|startoftext|>\": 66666,\n",
        "}\n",
        "\n",
        "rex_tkr = RegexTokenizer(special_tokens=special_tokens,\n",
        "                         split_pattern=GPT4_SPLIT_PATTERN)\n",
        "rex_tkr.train(\"abc! abc! abc! hello hello world steven\", 257, verbose=True)\n",
        "rex_tkr.encode(\"<|startoftext|>abc!<|endoftext|>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG9T1yqS1xZu",
        "outputId": "97dec859-9637-477b-b341-7eedbd21af08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merge (b'a', b'b') -> b'ab' | (97, 98) -> 256\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[66666, 256, 99, 33, 55555]"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    }
  ]
}